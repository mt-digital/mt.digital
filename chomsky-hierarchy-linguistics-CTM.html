<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Matthew Turner" />
  <title>Chomsky Hierarchy & CogSci</title>
  <style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
          href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.4/css/bootstrap.min.css"
          integrity="2hfp1SzUoho7/TsGGGDaFdsuuDL0LX2hnUp6VkX3CUQ2K4K+xjboZdsXyp4oUHZj"
          crossorigin="anonymous">
</head>
<body>
<div class="container">
    <div id="header">
    <h1 class="title">The Chomsky Hierarchy, Linguistics, and Computational Theories of Mind
</h1>
    <br>
    <h2 class="author">Matthew Turner</h2>
    <h3 class="date">September 13, 2016</h3>
    <a href="/">Go back</a>
    </div>
    <br>
    <hr>
    <br>
    <h1 id="introduction-the-chomsky-hierarchy"><span class="header-section-number">1</span> Introduction: The Chomsky Hierarchy</h1>
    <p>What is the dimensionality of language? Is there a “standard model” of language, where if we bring about the right technologies we might split the metaphorical atom of language? Chomsky believes so, a belief which is exemplified by his early work on the hierarchies of language generation, which he believes to be universal and written in our DNA <span class="citation">(Chomsky &amp; Gliedman, 1983)</span>. Each of the four levels of the hierarchy is a grammar that can be associated with a linguistic interpretation, or in other words associated with a class of grammars with certain properties. Each level of the hierarchy also comes with an associated class of automata and decidability characteristics which are summarized below. As important as the creation (or some would say discovery) of the hierarchy is the creation of a formal system for describing grammars <span class="citation">(Chomsky, 1956, <span class="citation">Chomsky (1959)</span>)</span>. This system has become the foundation for the study of grammar in languages that fit the prototype of Noun/Verb/Adjective/etc. that we see in English. However it also provides for the discovery of the fascinating group structures present in the Context-Free (CF) grammars, which are the basis for most computer languages <span class="citation">(Chomsky &amp; Schützenberger, 1963)</span>.</p>
    <p>All lower levels of the hierarchy are more specific instances of the most general set of grammers, the “recursively enumerable”, “Type-0”, or “unrestricted” grammars. We will describe the Type-0 grammar first and then its more specific descendents. Each grammar can be succinctly described in terms of what sort of computational machinery is required to determine whether or not a particular string is a member of a language generated by a grammar of the particular class of grammars.</p>
    <ul>
    <li><strong>Type-0 Grammars (“unrestricted”):</strong> A grammar that does not fit into one of the simpler grammars given below can only be parsed by a Turing machine. That is, an apparatus with a state of infinite memory with unrestricted access to locations in memory.</li>
    <li><strong>Type-1 Grammars (“context-sensitive”):</strong> These require random-access memory to establish membership in a given context-sensitive language. Polysemy in English reveals potential context sensitivity. For example, the meaning of the word “fly” might change depending on whether someone is taking trip by airplane or getting out the fly swatter. In programming languages, context often indicates the scope, so that in a particular program a symbol of the same name can point to two different locations in memory as long as the two symbols are not in the same scope.</li>
    <li><strong>Type-2 Grammars (“context-free”):</strong> These grammars generate languages that require only a stack memory system to establish membership of a particular string. As such, we can see that XML, which is based on tags like <code>&lt;el&gt;&lt;data&gt;Data here!&lt;/data&gt;&lt;/el&gt;</code>, requires only a stack memory to parse, and so the XML grammar is context-free.</li>
    <li><strong>Type-3 Grammars (“regular”):</strong> These require no memory to implement. Regular grammars simply generate a finite number of transformations of source strings, as in regular expressions. These can also be represented by fininte-state automata, which transition from one state to another without explicit dependency on past states, only the current input state.</li>
    </ul>
    <h1 id="impact-of-the-chomsky-hierarchy"><span class="header-section-number">2</span> Impact of the Chomsky Hierarchy</h1>
    <h2 id="in-linguistics"><span class="header-section-number">2.1</span> In Linguistics</h2>
    <p>In an interview, Chomsky himself described his contributions to linguistics as “sort of pre-Galilean” <span class="citation">(Chomsky &amp; Gliedman, 1983)</span>. What he means is that while he may have made a major contribution to linguistics, it is like he overturned the geocentric theory of the Universe and revealed that there is another way. We discuss this more in the next paragraph, but Chomsky revealed the <em>generational</em> nature of language. From simple rules, it’s possible to model grammatically-correct English. Chomsky is not interested in the psychology of linguistic performance nor the finer points of language generation. Indeed, he clearly states in his initial papers that his aim is not to reproduce human speech, but instead to provide a relatively simple model that can generate grammatical English speech. He does this and also shows that, contrary to some people’s intuitive beliefs of the time,</p>
    <blockquote>
    <p>no finite-state Markov process that produces symbols with transition from state to state can serve as an English grammar.</p>
    </blockquote>
    <h2 id="in-computer-science"><span class="header-section-number">2.2</span> In Computer Science</h2>
    <p>We get computers to do what we want by writing programs in a particular programming language. The Chomsky hierarchy provides a foundation for reasoning about how to specify the grammar of a particular programming language. A specific grammar is required so that compilers and interpreters may be written that translate a user’s code to processor instructions. Chomsky’s hierarchy also addresses “decidability” problems, which boil down to whether or not a sentence <span class="math inline"><em>S</em></span> belongs to a language generated by a particular grammar, typically written as <span class="math inline"><em>L</em>(<em>G</em>)</span> <span class="citation">(Chomsky, 1956, <span class="citation">Chomsky (1959)</span>)</span>. Decidability has further analogies in computability theory.</p>
    <h1 id="the-chomsky-hierarchy-is-all-about-competence"><span class="header-section-number">3</span> The Chomsky Hierarchy is all about “Competence”</h1>
    <p>Chomsky believes that the rules he was uncovering about the grammatical generation of language are “universal.” He adopts the mathematician’s view that theorems are not inventions, but discoveries. As such, all the rich mathematical structure that results from this “Universal Grammar” as he calls it is ingrained not only in the DNA of language producers, but etched in the universe itself. As such, following this view, the Chomsky hierarchy would continue to “exist” without humans just like the density of real numbers  will continue to exist at least as long as this Universe exists <span class="citation">(Tomasello, 2005, <span class="citation">Chomsky &amp; Schützenberger (1963)</span>)</span>.</p>
    <p>Not surprisingly, those most involved, accustomed and sensitive to the many subtleties and “edge cases” of human language find plenty of issues with the Chomsky language formalism because some languages cannot actually be formalized in that way. For example, <span class="citation">Tomasello (2005)</span> points out that the actual <em>performance</em> of language in the wild includes languages that are not <em>accusative languages</em>. For example, there are <em>ergative languages</em> in which “the notion of ‘subject’ does not operate like it does in English and other accusative languages, and so a direct connection to agent is not possible” <span class="citation">(Tomasello, 2005, p. 186)</span>. So not only is the Chomsky hierarchy related to competence, but perhaps it is a competence whose performance is only observable in a subset of all 6000-plus known languages spoken on Earth.</p>
    <h1 id="symbolic-systems-and-the-hierarchy"><span class="header-section-number">4</span> Symbolic Systems and the Hierarchy</h1>
    <p>With regards to various symbolic systems theories of cognition, such as <span class="citation">Rescorla (2015)</span> which discusses computational theories of mind (CTM) or the language of thought (LOT) prgram described in <span class="citation">Schneider &amp; Katz (2012)</span>, there is at least one immediate consequence of the hierarchy. That one consequence is that the system of symbols is a language itself. Therefore, it is important to understand where in the hierarchy the symbolic system finds itself, as this will, in turn, reflect the complexity of the set of possible mental representations (the language) and the hardware which is required to process such mental representations. As an absurd example, it’s clear there’s no way to have a Turing machine as a brain, so if we limit ourselves to the levels of Chomsky’s original hierary</p>
    <p><span class="citation">Jäger &amp; Rogers (2012)</span> consider a more “refined” hierarchy with more gradiation than the four original levels from <span class="citation">Chomsky (1956)</span>. For example, they find that an even simpler structure than stack memory can decide membership in the context-free languages for a particular subset of the context-free languages, namely the language <span class="math inline"><em>L</em></span> given generally by <span class="math inline"><em>a</em><sup><em>n</em></sup><em>b</em><sup><em>n</em></sup></span>. In this case, according to the authors, membership of a string in <span class="math inline"><em>L</em></span> can be determined using a finite-state automaton “augmented with a counter”. Even slightly more complex context-free languages do not allow decision of belonging to the context-free languages. This infrastructure is used in a subsequent paper, <span class="citation">Öttl, Jäger, &amp; Kaup (2015)</span>, to show that the behavior of experimental subjects in artificial grammar learning (AGL) experiments <em>did not</em> show that their faculties were forced into more complex states along with increasing complexity of grammars along the “refined” Chomsky hierarchy <span class="citation">Jäger &amp; Rogers (2012)</span> had previously developed. Although infants may not be able to perform language at first, this work would seem to suggest that in cognitive terms, there is not a corresponding increase in complexity as humans comprehend and generate sentences farther and farther up the Chomsky hierarchy.</p>
    <p>What about language faculties of non-human animals? Where might we place these on the “refined” Chomsky hierarchy? <span class="citation">Hauser, Chomsky, &amp; Fitch (n.d.)</span> provide an in-depth review of what they call the faculty of language in the broad sense (FLB) and faculty of language in the narrow sense (FLN) in both human and non-human animals. “At a minimum…FLN includes the capacity of recursion,” according to the authors. FLB on the other hand is more dynamic, as it “includes an internal computation system,” (FLN), “combined with at least two other organism-internal systems, which we call ‘sensory-motor’ and ‘conceptual-intentional’”. The authors posit a set of hypotheses, but proving any one is difficult because, as the authors admit, “each of these hypotheses is plausible to some degree. Ultimately, they can be distinguished only by empirical data, much of which is currently unavailable.” But, once that data is in hand, we may begin to assign different instances of non-human communication to particular groups in either the Chomsky hierarchy itself or some more refined version of the Chomsky hierarchy.</p>
    <h1 id="references" class="unnumbered">References</h1>
    <div id="refs" class="references">
    <div id="ref-Chomsky1956">
    <p>Chomsky, N. (1956). Three models for the description of language. <em>IRE Transactions on Information Theory</em>, <em>2</em>(3), 113–124. <a href="http://doi.org/10.1109/TIT.1956.1056813" class="uri">http://doi.org/10.1109/TIT.1956.1056813</a></p>
    </div>
    <div id="ref-Chomsky1959">
    <p>Chomsky, N. (1959). On Certain Properties of Formal Grammars. <em>Information and Control</em>, <em>2</em>(2), 137–167. <a href="http://doi.org/10.1016/S0019-9958(59)90362-6" class="uri">http://doi.org/10.1016/S0019-9958(59)90362-6</a></p>
    </div>
    <div id="ref-Chomsky1983">
    <p>Chomsky, N., &amp; Gliedman, J. (1983). Things no amount of learning can teach. <em>Omni</em>. Retrieved from <a href="https://chomsky.info/198311__" class="uri">https://chomsky.info/198311__</a></p>
    </div>
    <div id="ref-Chomsky1963">
    <p>Chomsky, N., &amp; Schützenberger, M.-P. (1963). The Algebraic Theory of Context-Free Languages. In P. Braffort &amp; D. Hirschberg (Eds.), <em>Computer programming and formal systems</em> (pp. 118–161). Amsterdam: North-Holland Publishing Company.</p>
    </div>
    <div id="ref-Hauser">
    <p>Hauser, M. D., Chomsky, N., &amp; Fitch, W. T. (n.d.). The Faculty of Language: What Is It, Who Has It, and How Did It Evolve?</p>
    </div>
    <div id="ref-Jager2012">
    <p>Jäger, G., &amp; Rogers, J. (2012). Formal language theory: refining the Chomsky hierarchy. <em>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</em>, <em>367</em>(1598), 1956–70. <a href="http://doi.org/10.1098/rstb.2012.0077" class="uri">http://doi.org/10.1098/rstb.2012.0077</a></p>
    </div>
    <div id="ref-Ottl2015">
    <p>Öttl, B., Jäger, G., &amp; Kaup, B. (2015). Does formal complexity reflect cognitive complexity? Investigating aspects of the Chomsky Hierarchy in an artificial language learning study. <em>PLoS ONE</em>, <em>10</em>(4), e0123059. <a href="http://doi.org/10.1371/journal.pone.0123059" class="uri">http://doi.org/10.1371/journal.pone.0123059</a></p>
    </div>
    <div id="ref-Rescorla2015">
    <p>Rescorla, M. (2015). Computational modeling of the mind: What role for mental representation? <em>Wiley Interdisciplinary Reviews: Cognitive Science</em>, <em>6</em>(1), 65–73. <a href="http://doi.org/10.1002/wcs.1325" class="uri">http://doi.org/10.1002/wcs.1325</a></p>
    </div>
    <div id="ref-Schneider2012">
    <p>Schneider, S., &amp; Katz, M. (2012). Rethinking the language of thought. <em>Wiley Interdisciplinary Reviews: Cognitive Science</em>, <em>3</em>(2), 153–162. <a href="http://doi.org/10.1002/wcs.1155" class="uri">http://doi.org/10.1002/wcs.1155</a></p>
    </div>
    <div id="ref-Tomasello">
    <p>Tomasello, M. (2005). Beyond formalities: The case of language acquisition. <a href="http://doi.org/10.1515/tlir.2005.22.2-4.183" class="uri">http://doi.org/10.1515/tlir.2005.22.2-4.183</a></p>
    </div>
    </div>
</div>
</body>
</html>
