<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<link rel="stylesheet"
          href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.4/css/bootstrap.min.css"
          integrity="2hfp1SzUoho7/TsGGGDaFdsuuDL0LX2hnUp6VkX3CUQ2K4K+xjboZdsXyp4oUHZj"
          crossorigin="anonymous">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Matthew Turner" />
  <title>Notes on Corpus Building</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
</head>
<body>

<div class="container">
    <div id="header">
        <h1 class="title">Notes on Corpus Building</h1>
        <br>
        <h2 class="author">Matthew Turner</h2>
        <h3 class="date">October 7, 2016</h3>
        <a href="/">Go back</a>
    </div>

    <br>
    <hr>
    <br>

    <h1 id="the-situation">The situation</h1>
    <p>After struggling a bit to get some better direction for my work, I realized I had abandoned corpus building in favor of studying figurative language. This isn’t a bad thing in itself, and I need to study figurative language more (more specifically I’ve been reading <span class="citation">Dancygier and Sweetser (2014)</span> and looking through <span class="citation">Lakoff (1987)</span> and <span class="citation">Gibbs (1994)</span>, but only cursorily since there is so much other reading to be done for foundational cognitive science at this time. To return to building this corpus, and eventually analytics to make it “corpus plus”, is to return to my own wheelhouse where I can make some good progress. This is also the goal for my upcoming talk at CIS on October 31, 2016.</p>
    <h1 id="towards-a-prototype">Towards a prototype</h1>
    <p>The immediate source I’d like to incorporate is the <a href="http://archive.org/details/tv">TV Cable News Archive</a>. After that, the 2012 and 2016 debates, transcripts of which are available at <a href="http://debates.org">debates.org</a>. For the first round of analytics, I want to make an n-gram viewer, like <a href="https://books.google.com/ngrams">Google Books Ngram Viewer</a>. Instead of a corpus of books, I want to use a corpus that will be updated regularly based on the ten or so shows we decide on from across the political spectrum. The user will be able to select which datasets (broken down by type, station if the type is cable news program, date range, etc) go into their ngrams that are displayed. I was thinking about how to do this efficiently and it occurred to me that the occurrence frequency could be computed on a per-document basis. Then, if we also index the number of words in each document we can efficiency calculate the weighted average of all component frequencies for the datasets of interest. There is a module in <code>nltk</code> for generating ngrams with a simple example, and some native python examples, <a href="http://stackoverflow.com/questions/17531684/n-grams-in-python-four-five-six-grams">on Stack Overflow</a>. Here is a way we can do this:</p>
    <div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> nltk <span class="im">import</span> ngrams
    sentence <span class="op">=</span> <span class="st">&#39;this is a foo bar sentences and i want to ngramize it&#39;</span>
    n <span class="op">=</span> <span class="dv">6</span>
    sixgrams <span class="op">=</span> ngrams(sentence.split(), n)
    <span class="cf">for</span> grams <span class="op">in</span> sixgrams:
      <span class="bu">print</span> grams</code></pre></div>
    <p>Alternatively, the <a href="http://textacy.readthedocs.io/en/latest/">textacy Python package</a> may be a more modern alternative to NLTK. It has a lot of functionality at the fore, including cleaning/normalizing, named entity recognition, direct quotation extraction, display kewords in context, transform documents and corpora into vectorized and semantic network representation, and compare documents and do topic modeling with a variety of methods.</p>
    <p>Another dataset that might be interesting would be the Congressional Record, made available through the Sunlight Foundation’s <a href="https://sunlightlabs.github.io/Capitol-Words/">Capitol Words API</a>. They have an assoicated <a href="http://capitolwords.org/?terma=global+warming&amp;termb=climate+change">webapp</a> that itself is a Google Ngram clone. They do a similar breakdown like I propose, where they further decompose the n-gram graph into political parties, and show the most recent documents where the word has occurred. Unlike the Google app, the user is not able to compare multiple n-grams.</p>
    <h1 id="references" class="unnumbered">References</h1>
    <div id="refs" class="references">
    <div id="ref-Dancygier2014">
    <p>Dancygier, Barbara, and Eve Sweetser. 2014. <em>Figurative Language</em>. Cambridge: Cambridge University Press.</p>
    </div>
    <div id="ref-Gibbs1994">
    <p>Gibbs, Raymond W. 1994. <em>The Poetics of Mind</em>. New York: Cambridge University Press.</p>
    </div>
    <div id="ref-Lakoff1987">
    <p>Lakoff, George. 1987. <em>Women, Fire, and Dangerous Things</em>. Chicago: University of Chigago Press.</p>
    </div>
    </div>
</div>
</body>
</html>
